# seq, gnn, fbnetgen 
name: BrainNetworkTransformer
dim_feedforward: 1024 #1024 in original
nhead: 4
dim_reduction: 8
sizes: [200, 100]  # Note: The input node size should not be included here, number of elements equal to number of attention layers
pooling: [false, true]
pos_encoding:
 name: RRWPEncoding  # IdentityEncoding, RRWPEncoding, none
 embed_dim: 32 #32 number of features + node numbers should divide 2 4
orthogonal: true
freeze_center: true
project_assignment: true
forward_inputs: ["node_feature"]